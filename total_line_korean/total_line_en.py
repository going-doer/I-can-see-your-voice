# -*- coding: utf-8 -*-
"""total_line_en.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GncYyXZcBj1i3Se4AtyZEiCWPQiulB3M

## Setting

### 필요 파일 다운로드
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install moviepy #음원추출을 위함
# !pip install pydub #mp3 -> wav
# 
# !apt install ffmpeg #음성파일 형변환을 위해
# 
# !pip install imageio-ffmpeg
# !pip install imageio==2.4.1
# 
# !pip install pyannote.database
# !pip install pyannote.metrics
# !pip install pyannote.pipeline
# !pip install pyannote.core
# !pip install pyannote.audio
# 
# !pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 torchtext==0.12.0
# 
# !pip install datasets==1.18.3
# !pip install transformers==4.17.0
# !pip install torchaudio
# !pip install librosa
# !pip install jiwer
# !pip install git+https://github.com/huggingface/datasets.git
# !pip install git+https://github.com/huggingface/transformers.git
# 
# !pip install ipympl

"""### import"""

# 4. ASS
import json
import codecs
import pandas
import re,time

# load the pipeline from Hugginface Hub
from pyannote.audio import Pipeline

# 영상 컷편집
from pydub import AudioSegment
# import IPython.dis/play as ipd
import numpy as np
import random
import librosa
import pandas as pd

# SER & ASR
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
from transformers import AutoConfig, Wav2Vec2Processor

import librosa
import numpy as np
import pandas as pd

from transformers import Wav2Vec2ForCTC

from dataclasses import dataclass
from typing import Optional, Tuple
from transformers.file_utils import ModelOutput

from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
from transformers.models.wav2vec2.modeling_wav2vec2 import (
    Wav2Vec2PreTrainedModel,
    Wav2Vec2Model
)

# ASS
import os
from pathlib import Path

# STT model

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name_or_path_stt = "facebook/wav2vec2-large-960h-lv60-self"
config_stt = AutoConfig.from_pretrained(model_name_or_path_stt)
processor_stt = Wav2Vec2Processor.from_pretrained(model_name_or_path_stt)
sampling_rate_stt = processor_stt.feature_extractor.sampling_rate
model = Wav2Vec2ForCTC.from_pretrained(
    model_name_or_path_stt,
    attention_dropout=0.1,
    hidden_dropout=0.1,
    feat_proj_dropout=0.0,
    mask_time_prob=0.05,
    layerdrop=0.1,
    ctc_loss_reduction="mean", 
    pad_token_id=processor_stt.tokenizer.pad_token_id,
    # vocab_size=len(processor.tokenizer) # vocab_size에서 걸리면 요거 추가해주면됨
)

## SER & ASR

@dataclass
class SpeechClassifierOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
  

class Wav2Vec2ClassificationHead(nn.Module):
    """Head for wav2vec classification task."""

    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, features, **kwargs):
        x = features
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.pooling_mode = config.pooling_mode
        self.config = config

        self.wav2vec2 = Wav2Vec2Model(config)
        self.classifier = Wav2Vec2ClassificationHead(config)

        self.init_weights()

    def freeze_feature_extractor(self):
        self.wav2vec2.feature_extractor._freeze_parameters()

    def merged_strategy(
            self,
            hidden_states,
            mode="mean"
    ):
        if mode == "mean":
            outputs = torch.mean(hidden_states, dim=1)
        elif mode == "sum":
            outputs = torch.sum(hidden_states, dim=1)
        elif mode == "max":
            outputs = torch.max(hidden_states, dim=1)[0]
        else:
            raise Exception(
                "The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']")

        return outputs

    def forward(
            self,
            input_values,
            attention_mask=None,
            output_attentions=None,
            output_hidden_states=None,
            return_dict=None,
            labels=None,
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        outputs = self.wav2vec2(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = outputs[0]
        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)
        logits = self.classifier(hidden_states)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SpeechClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

# SER model download

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name_or_path_emotion = "jungjongho/wav2vec2-xlsr-korean-speech-emotion-recognition2"
config_emotion = AutoConfig.from_pretrained(model_name_or_path_emotion)
processor_emotion = Wav2Vec2Processor.from_pretrained(model_name_or_path_emotion)
sampling_rate_emotion = processor_emotion.feature_extractor.sampling_rate
model_emotion = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path_emotion).to(device)

"""## 1. 영상 .mp4 -> wav 변환"""

name=input('파일 이름을 입력하세요 (mp4제외): ')

import moviepy.editor as mp

clip = mp.VideoFileClip("./mp4_data/{}.mp4".format(name))
clip.audio.write_audiofile("./wav_data/{}.wav".format(name)) #wa파일로 변환

"""## 2. wav -> array wav파일을 array형태로 바꾸기"""

# wav 파일 array 형태로 변환
speech_array, sampling_rate = librosa.core.load("./wav_data/{}.wav".format(name), sr=16000) #wav->array

# 파일 길이 구하기(초)
wav_sec=len(speech_array)/sampling_rate

# 모델 불러와서 실행 후 1초 이상인 것들만 저장
pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization')

diarization = pipeline("./wav_data/{}.wav".format(name))
diarization.for_json()['content']
   
df=pd.DataFrame(diarization.for_json()['content'])
df['len']=df['segment'].apply(lambda x: x['end']-x['start'])

df=df[df['len']>1]

"""## 3. 영상 화자별로 자르기

"""

#필요한 코드인지 여쭤보기 !

start_sec=0 #시작 
finsh_sec=wav_sec #끝

data={}

wav2_sec=finsh_sec-start_sec

per2_sec = len(speech_array)/wav2_sec


for idx, i in enumerate(df.iterrows()):
  data_tmp={}
  data_tmp['audio']=speech_array[round(per2_sec*i[1][0]['start']):round(per2_sec*i[1][0]['end'])]
  data_tmp['track']=i[1][1]
  data_tmp['label']=i[1][2]

  data['{}'.format(idx)]=data_tmp

# 화자별 시간대로 자르기
wav2_sec=finsh_sec-start_sec
per2_sec = len(speech_array)/wav2_sec

wav_data_list=[]
trash=[]

for idx, i in enumerate(df['segment']) :
  if round(per2_sec*i['end'])-round(per2_sec*i['start'])<=1:
      trash.append(idx)
  else:
      globals()["y_{}".format(idx)]=speech_array[round(per2_sec*i['start']):round(per2_sec*i['end'])]
      wav_data_list.append(globals()["y_{}".format(idx)])

df['wav_array']=wav_data_list

# 각자 자른 wav 파일 저장
# 컷파일 올리기
# split_file 파일 만들어주기

import soundfile as sf

file_path=[]

for idx, i in enumerate(df.iterrows()):
    print('파일나누는중')
    sf.write('./split_file/test_{}.wav'.format(idx), i[1][4], 16000, ) 
    file_path.append('./split_file/test_{}.wav'.format(idx))
  

df['path']=file_path
df.to_csv('./split_file/file_information.csv')

print('파일에 나눠서 저장함')

"""## 4. 각자 자른 wav파일  -> STT"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name_or_path_stt = "facebook/wav2vec2-large-960h-lv60-self"
config_stt = AutoConfig.from_pretrained(model_name_or_path_stt)
processor_stt = Wav2Vec2Processor.from_pretrained(model_name_or_path_stt)
sampling_rate_stt = processor_stt.feature_extractor.sampling_rate
model = Wav2Vec2ForCTC.from_pretrained(model_name_or_path_stt)

# produce stt file
text=[]
for i in df.iterrows():
    input_dict = processor_stt(i[1][4], return_tensors="pt", padding="longest")
    logits = model(input_dict.input_values).logits    

    pred_ids = torch.argmax(logits, dim=-1)[0]
    pred_str = processor_stt.decode(pred_ids)
    
    pred_str = pred_str.lower()
    text.append(pred_str)

df['text']=text

"""## 5. wav파일을 통한 감정분석"""

# dataclass
@dataclass
class SpeechClassifierOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None

# model 불러오기
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name_or_path_emotion = "jungjongho/wav2vec2-xlsr-korean-speech-emotion-recognition2"
config_emotion = AutoConfig.from_pretrained(model_name_or_path_emotion)
processor_emotion = Wav2Vec2Processor.from_pretrained(model_name_or_path_emotion)
sampling_rate_emotion = processor_emotion.feature_extractor.sampling_rate
model_emotion = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path_emotion).to(device)

config_emotion.id2label[0] = '기쁨'
config_emotion.id2label[1] = '당황'
config_emotion.id2label[2] = '분노'
config_emotion.id2label[3] = '불안'
config_emotion.id2label[4] = '슬픔'
config_emotion.id2label[5] = '중립'

def speech_file_to_array_fn(path, sampling_rate):
    speech, _ = librosa.core.load(path, sr=16000)
    return speech


def predict(wav, sampling_rate):
    emotion_label = '';
    features_emotion = processor_emotion(wav, sampling_rate=sampling_rate, return_tensors="pt", padding=True)

    input_values = features_emotion.input_values.to(device)
    attention_mask = features_emotion.attention_mask.to(device)

    with torch.no_grad():
        logits = model_emotion(input_values, attention_mask=attention_mask).logits

    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]

    outputs = [{config_emotion.id2label[i]: round(score * 100, 3)} for i, score in enumerate(scores)]
    print(outputs)
    return outputs

# 감정 추출 후 감정 비율 높은 순으로 정렬
import operator

emotion_list=[]
trash_idx=[]

for idx,i in enumerate(df.iterrows()):
  try:
    emotion_list.append(predict(i[1][4],16000))
  except:
    trash_idx.append(idx)


for i in range(len(emotion_list)):
  emotion_sorted = [] # 감정 비율 높은 순으로 정렬된 리스트
  temp_dict = {}  # 정렬을 위한 임시 저장 딕셔너리

  for emotion in emotion_list[i]:
    for k, v in emotion.items():
      temp_dict[k] = v

  emotion_sorted = sorted(temp_dict.items(), key=operator.itemgetter(1),reverse=True)

  emotion_list[i] = emotion_sorted

# 감정 카테고리에 정렬된 값 넣어주기
df['emotion']=emotion_list

"""## 6. ass 자막 생성"""

# 데이터 프레임 가져오기
audio_df = df

# 쓸모없는 정보들 삭제
del audio_df['track']
del audio_df['len']
del audio_df['wav_array']

# 인덱스 초기화 - 기존 인덱스 순서대로 안되어있어서 맞춤
audio_df.reset_index(drop = False, inplace = True)

# 기존 인덱스 열 삭제
del audio_df['index']

# 인덱스를 기준으로 딕셔너리 생성
audio_dt = audio_df.to_dict('index')

# 4. ASS
class Assem2Ass(object):
    def __init__(self,assem_dict):
        """
        A class to convert dict_data_assem to ASS format
        """
        # Base screen size for placement calculations.
        # Everything scales according to these vs actual.
        # Font size and Shadow/Outline pixel widths apply to this screen size.
        self.width = 100
        self.height = 100
        # Subtitle events,styles are a dict
        self.events = {}
        self.styles = {}
        # Headers for each section of the ASS file
        # TODO: Add more Script Info
        # ass 자막 필수요소 [Script Info], [V4 Styles], [Events]
        self.Script_Info = "[Script Info]\n" \
        "ScriptType: V4.00+\nWrapStyle: 0\nScaledBorderAndShadow: yes\n"\
        "YCbCr Matrix: TV.601\nPlayResX: 1920\nPlayResY: 1080\n"
        self.V4_Styles = "[V4+ Styles]\n"\
        "Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour,"\
        "Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing,"\
        "Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n"
        self.Events = "[Events]\n"\
        "Format: Layer, Start, End, Style, Name, " \
        "MarginL, MarginR, MarginV, Effect, Text\n"
        #   self.df=pandas.DataFrame(assem_dict)
        self.dt=assem_dict
        self._parse_assem()
        self._convert_to_ass()
    
    # TODO : 감정벡터를 자막 그래픽으로 구현
    # 폰트 색상만 변경 - 구체화 필요
    def emo2vec(self,emotion):
        f_e=emotion[0][0]
        f_e_v=emotion[0][1]
       
        font_size='50'
        outline='2'
        shadow='2'
        font_name='Arial'
        fgColor='&H00FFFFFF'
        bgColor='&H00FFFFFF'
        
        if f_e=='중립':
            font_name='Arial'
            fgColor='&H00FFFFFF'    # 흰색
        elif f_e=='기쁨':
            font_name='평창 평화체'
            fgColor='&H0000FFFF'    # 노랑
        elif f_e=='슬픔':
            font_name='돋움'
            fgColor='&H00FF0000'    # 파랑
            outline='10'
            shadow='10'
        elif f_e=='불안':
            font_name='굴림'
            fgColor='&H0000FF00'    # 초록
            outline='0'
            shadow='10'
        elif f_e=='당황':
            font_name='바탕'
            fgColor='&H00FF00C8'    # 보라            
        elif f_e=='분노':
            font_name='궁서'
            font_size='55'
            fgColor='&H000000FF'    # 빨강
            outline='10'
            shadow='2'
        # print(font_name,font_size,fgColor,bgColor,outline,shadow)    
        return font_name,font_size,fgColor,bgColor,outline,shadow
    
    # assem에 있는 text + emotion 데이터 파싱 후 ass 형식으로 바꾸어 저장
    def _parse_assem(self):
        """
        Convert the input assem into separate dicts for
        events (text + duration) and styles to be applied to those events.
        """
        data=self.dt
        for index_dt in range(len(data)):
                t_start=self.t_process(data[index_dt]['segment']['start'])
                t_end=self.t_process(data[index_dt]['segment']['end'])
                text=data[index_dt]['text']

                first_emotion=data[index_dt]['emotion'][0][0]
                
                font_name,font_size,fgColor,bgColor,outline,shadow=self.emo2vec(data[index_dt]['emotion'])                
                # print(font_name,font_size,fgColor,bgColor,outline,shadow)
                
                speaker_name = data[index_dt]['label']
                speaker_name_emotion = data[index_dt]['label']+'_'+first_emotion
                
                self.events.update({
                    index_dt: {"Text": self.line_shift(speaker_name+':'+text,font_size), "Start": t_start, "End": t_end,
                          "Style":speaker_name_emotion}
                })
                self.styles.update({
                    speaker_name_emotion: {'Fontname':font_name,'Fontsize':font_size,"PrimaryColour": fgColor,
                                           "BackColour": bgColor, 'Outline':outline, 'Shadow':shadow}
                })
            
                
    def line_shift(self,text,font_size):
        text_list=[]
        max_chars=int(int(font_size))
        result=''
        if int(len(text)/max_chars)>0:
            
            for i in  range(int(len(text)/max_chars)+1):
                 text_list.append(text[(i)*max_chars:(i+1)*max_chars]+'\\N')
                 # print(text_list[i])
                 result+=text_list[i]
        else:
            result=text
        print(result)
        return result


           
    # ass 파일로 변환     
    def _convert_to_ass(self):
        self._write_styles()
        self._write_events()
        # print(self.styles)
    
    def _write_styles(self):
        """
        Write out the style information to self.V4_Styles
        """
        # 변경이 불필요한 기타 정보 초기화
        misc_data = {
            # 'Fontname': 'Arial',
            # 'Fontsize': '100',
            # 'PrimaryColour':'&H00000000',
            'SecondaryColour': '&H000000FF',
            'OutlineColour': '&H00000000',
            # 'BackColour': '&H00000000',
            'Bold': '0',
            'Italic': '0',
            'Underline': '0',
            'StrikeOut': '0',
            'ScaleX': '100',
            'ScaleY': '100',
            'Spacing': '0',
            'Angle': '0',
            'BorderStyle': '1',
            # 'Outline': '2',
            # 'Shadow': '2',
            'Alignment': '2',
            'MarginL': '10',
            'MarginR': '10',
            'MarginV': '10',
            'Encoding': '1',
        }
        
        for (name, data) in self.styles.items():
            data.update(misc_data)
            line = u"Style: {Name},{Fontname},{Fontsize},{PrimaryColour}," \
            "{SecondaryColour},{OutlineColour},{BackColour},{Bold}," \
            "{Italic},{Underline},{StrikeOut},{ScaleX},{ScaleY},{Spacing},{Angle},"\
            "{BorderStyle},{Outline},{Shadow},{Alignment}," \
            "{MarginL},{MarginR},{MarginV},{Encoding}" \
            "\n".format(Name=name, **data)
            self.V4_Styles += line

    def _write_events(self):
        """
        Write out subtitle event information to self.Events
        """
        misc_data = {
            'Layer': '0',
            'Name': '',
            'MarginL': '0',
            'MarginR': '0',
            'MarginV': '0',
            'Effect': '',
        }
        for (num, data) in self.events.items():
            data.update(misc_data)
            line = u"Dialogue: {Layer},{Start},{End},{Style}," \
            "{Name},{MarginL},{MarginR},{MarginV},{Effect},{Text}" \
            "\n".format(**data)
            self.Events += line

    def save(self, filename):
        with codecs.open(filename, 'w', encoding='utf8') as f:
            f.write(self.Script_Info)
            f.write("\n")
            f.write(self.V4_Styles)
            f.write("\n")
            f.write(self.Events)
            f.write("\n")  
        
            
    def check_ass(self):
        print(self.Script_Info)
        print(self.V4_Styles)
        print(self.Events)

    def t_process(self,idx_time):
    
        time=float(idx_time)
        hour=int(time/3600)
        minute=int(time%3600/60)
        second=int(time%3600%60)
        h_s=(time%3600%60-second)*100
        h1=str(hour).zfill(2)
        m1=str(minute).zfill(2)
        s1=str(second).zfill(2)
        f1=str(int(h_s)).zfill(2)

        return '{0}:{1}:{2}.{3}'.format(h1,m1,s1,f1)

# ass 자막 저장할 파일 이름
filename = 'test1.mp4'

# 실행 코드 (audio_dt가 위에서 생성한 데이터 딕셔너리에요)
ass=Assem2Ass(audio_dt)
save_dir = './ass_data/'
ass.save(os.path.join(save_dir, filename.replace(".mp4", "") + ".ass"))

# 생성된 ass 파일 체크

ass.check_ass()

